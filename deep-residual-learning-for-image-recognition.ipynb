{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Residual Learning for Image Recognition\n",
    "\n",
    "## Degradation Problem\n",
    "\n",
    "Deep convolutional neural networks가 나온 이후로 많은 발전이 있었습니다. <br>\n",
    "기본적으로 deep networks는 features들을 스스로 low/mid/high level features로 나누며<a href=\"#ref01\">$ ^{[01]} $</a>, 근래의 모델들의 경우 layers층을 더 깊이 있게 쌓아서 features들의 levels을 더욱 세분화하고자하는 시도가 있으며<a href=\"#ref02\">$ ^{[02]} $</a>, ImageNet 챌린지에서 16개 또는 30개등의 layers를 사용하는 매우 깊은 모델들을 사용하기도 하였습니다.<a href=\"#ref03\">$ ^{[03]} $</a>\n",
    "\n",
    "단순히 layers를 더 많이 쌓으면 더 좋은 결과를 낼 것인가? 라는 질문에는.. 사실 문제가 있습니다.<br>\n",
    "이미 잘 알려진 vanishing/exploding gradients<a href=\"#ref04\">$ ^{[04]} $</a> <a href=\"#ref05\">$ ^{[05]} $</a>의 문제는 convergence자체를 못하게 만듭니다. <br>\n",
    "이러한 문제는 normalized initialization<a href=\"#ref06\">$ ^{[06]} $</a> <a href=\"#ref06\">$ ^{[04]} $</a> <a href=\"#ref07\">$ ^{[07]} $</a>, 그리고 intermediate normalization layers <a href=\"#ref08\">$ ^{[08]} $</a>에 의해서 다소 해결이 되어 수십층의 layers들이 SGD를 통해 convergence될 수 있도록 도와줍니다. \n",
    "\n",
    "Deeper networks를 사용할때 **degradation problem**이 발견되었습니다. degradation problem은 network의 depth가 커질수록 accuracy는 saturated (마치 뭔가 가득차서 현상태에서 더 진전이 없어져 버리는 상태)가 되고 degradation이 진행됩니다. 이때 degradation은 overfitting에 의해서 생겨나는 것이 아니며, 더 많은 layers를 넣을수록 training error가 더 높아집니다.<a href=\"#ref09\">$ ^{[09]} $</a> (만약 overfitting이었다면 training error는 매우 낮아야 합니다.)\n",
    "\n",
    "![title](images/resnet_training_test_error.png)\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<span style=\"color:#777777;\">CIFAR-10 데이터에 대한 training error(왼쪽) 그리고 test error(오른쪽) 그래프.<br>\n",
    "    20-layers 그리고 56-layers를 사용했으며, 더 깊은 네트워크일수록 training error가 높으며, 따라서 test error또한 높다.</span>\n",
    "</div>\n",
    "\n",
    "한가지 실험에서 이를 뒷받침할 근거를 내놓습니다.<br>\n",
    "shallow network에서 학습된 모델위에 다층의 layers를 추가적으로 쌓습니다. 이론적으로는 deeper 모델이 shallower 모델에 추가된 것이기 때문에 더 낮은 training error를 보여야 합니다. 하지만 학습된 shallower 모델에 layers를 더 추가시켜도, 그냥 shallow 모델보다 더 높은 training error를 보여줍니다.\n",
    "\n",
    "![title](images/resnet_shallow_deep_model.png)\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<span style=\"color:#777777;\">**Constructed Solution**<br>Shallower model(왼쪽) 그리고 Deeper model(오른쪽). <br>Shallower model로 부터 학습된 지식을 복사하고, Identity로서 layers를 추가하였다.<br>Deeper model은 shallower model과 비교하여 더 낮거나 같은 training error를 보여야 하지만 <br>실제는 degradation현상으로 인하여 layers가 깊어질수록 training error는 높아진다</span>\n",
    "</div>\n",
    "\n",
    "이러한 degradation 문제는 Deep residual learning<a href=\"#ref10\">$ ^{[10]} $</a>를 통해서 해결될 수 있습니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Neural Networks\n",
    "\n",
    "Deep Neural Network를 흔히 말할때 어떠한 복잡한 함수도 approximator $ y = f(x) $로서 역활을 할수 있기 때문에 \n",
    "identity function $ x = f(x) $ 또한 학습할수 있다고 말할 수 있습니다. 하지만 현실은 vanishing/exploding gradients이슈 그리고  curse of dimensionality problem으로 학습이 잘 안됩니다. \n",
    "\n",
    "ResNets은 이러한 문제를 해결하기 위하여,   강제로 Identity mapping (function)을 학습하도록 하였습니다.<br>\n",
    "Identity function은 단순히 $ id(x) = x $ 으로서, $ x $값을 받으면 동일한 $ x $를 리턴시킵니다.<br>\n",
    "어떤 subnetwork의 input을 $ x $ 라고 하고, true output은 $ H(x) $ 라고 가정합니다.<br>\n",
    "이때 residual값은 이 둘의 차이 입니다.\n",
    "\n",
    "$$ F(x) = H(x) - id(x) $$\n",
    "\n",
    "우리는 실제 true값을 알고자 하는 것이기 때문에 위의 공식은 다음과 같이 재정립할수 있습니다.\n",
    "\n",
    "$$ \\begin{align}\n",
    "H(x) &= F(x) + id(x)  \\\\\n",
    "&= F(x) + x\n",
    "\\end{align} $$\n",
    "\n",
    "일반적인 Neural Network는 $ H(x) $ 자체를 학습니다. \n",
    "\n",
    "![title](images/resnet_plain.png)\n",
    "\n",
    "ResNet의 경우는 다음과 같습니다.\n",
    "\n",
    "![title](images/resnet_resnet.png)\n",
    "\n",
    "ResNet의 경우는 Input과 subnetwork의 output의 차이(residual)을 학습합니다.<br>\n",
    "이러한 구조는 $ F(x) = 0 $를 만듬으로서 특정 subnetwork를 그냥 지나치도록 만듭니다. <br>\n",
    "쉽게 설명하면 $ H(x) = x $가 되기 때문에, 특정 subnetwork의 output이 그 다음 subnetwork의 output 그 자체가 되 버립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "인용한 문서들..\n",
    "\n",
    "* <a id=\"ref01\"></a> [01] [Visualizing and understanding convolutional neural networks](https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)\n",
    "* <a id=\"ref02\"></a> [02] [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n",
    "* <a id=\"ref03\"></a> [03]  [Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842.pdf)\n",
    "* <a id=\"ref04\"></a> [04] [Understanding the difficulty of training\n",
    "deep feedforward neural networks](http://www-prima.imag.fr/jlc/Courses/2016/PRML/XavierInitialisation.pdf)\n",
    "* <a id=\"ref05\"></a> [05] [Learning long-term dependencies\n",
    "with gradient descent is difficult](http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf)\n",
    "* <a id=\"ref06\"></a> [06] [Efficient backprop. In Neural Networks: Tricks of the Trade](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
    "* <a id=\"ref07\"></a> [07] [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](https://arxiv.org/pdf/1312.6120.pdf)\n",
    "* <a id=\"ref08\"></a> [08] [Batch normalization: Accelerating deep\n",
    "network training by reducing internal covariate shift](https://arxiv.org/abs/1502.03167)\n",
    "* <a id=\"ref09\"></a> [09] [Convolutional neural networks at constrained time cost](https://arxiv.org/abs/1412.1710)\n",
    "* <a id=\"ref10\"></a> [10] [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "\n",
    "글 쓰면서 참고한 문서들..\n",
    "\n",
    "* [Deep Residual Learning for Image Recognition - 원래 ResNet Paper](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "* [Identity Mappings in Deep Residual Networks - 개선된 Paper](https://arxiv.org/pdf/1603.05027.pdf)\n",
    "* [Residual neural networks are an exciting area of deep learning research](https://blog.init.ai/residual-neural-networks-are-an-exciting-area-of-deep-learning-research-acf14f4912e9)\n",
    "* [Deep Residual Networks ICML 2016 Tutorial](http://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
